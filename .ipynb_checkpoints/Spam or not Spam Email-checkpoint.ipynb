{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing important libraryes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "#from contractions import contractions_dict\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from itertools import filterfalse\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"email.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unwanted labels and remane the lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'label_num':'spam'}, inplace=True)\n",
    "data.drop('label',axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Unnamed: 0',axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unwanted words in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_titles(text):\n",
    "    if \"Subject: re :\" in text:\n",
    "        return text[13:]\n",
    "    elif \"Subject: news :\" in text:\n",
    "        return text[15:]\n",
    "    else:\n",
    "        return text[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: strip_titles(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We r going to tolonize all emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize = Now we have to convert all tokens into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens(list_of_tokens):\n",
    "    return map(lambda x: x.lower(),list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: normalize_tokens(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we r going to remove contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\"'cause\": 'because',\"'tis\": 'it is',\"'twas\": 'it was',\"I'd\": 'I would',\"I'd've\": 'I would have',\"I'll\": 'I will',\"I'll've\": 'I will have',\"I'm\": 'I am',\"I'm'a\": 'I am about to',\"I'm'o\": 'I am going to',\"I've\": 'I have','I’d': 'I would','I’d’ve': 'I would have','I’ll': 'I will','I’ll’ve': 'I will have','I’m': 'I am','I’m’a': 'I am about to','I’m’o': 'I am going to','I’ve': 'I have','Whatcha': 'What are you',\"ain't\": 'are not','ain’t': 'are not',\"amn't\": 'am not','amn’t': 'am not','apr.': 'april',\"aren't\": 'are not','aren’t': 'are not','aug.': 'august',\"can't\": 'cannot',\"can't've\": 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\"could've\": 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn’t': 'could not','couldn’t’ve': 'could not have','could’ve': 'could have',\"daren't\": 'dare not','daren’t': 'dare not',\"daresn't\": 'dare not','daresn’t': 'dare not',\"dasn't\": 'dare not','dasn’t': 'dare not','dec.': 'december',\"didn't\": 'did not','didn’t': 'did not',\"doesn't\": 'does not','doesn’t': 'does not',\"don't\": 'do not','don’t': 'do not',\"e'er\": 'ever','em': 'them',\"everyone's\": 'everyone is','everyone’s': 'everyone is','e’er': 'ever','feb.': 'february','finna': 'fixing to','gimme': 'give me',\"gon't\": 'go not','gonna': 'going to','gon’t': 'go not','gotta': 'got to',\"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn’t': 'had not','hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn’t': 'has not',\"haven't\": 'have not','haven’t': 'have not',\"he'd\": 'he would',\"he'd've\": 'he would have',\"he'll\": 'he will',\"he'll've\": 'he will have',\"he's\": 'he is',\"he've\": 'he have',\"here's\": 'here is','here’s': 'here is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’ll’ve': 'he will have','he’s': 'he is','he’ve': 'he have',\"how'd\": 'how did',\"how'd'y\": 'how do you',\"how'll\": 'how will',\"how're\": 'how are',\"how's\": 'how is','how’d': 'how did','how’d’y': 'how do you','how’ll': 'how will','how’re': 'how are','how’s': 'how is',\"isn't\": 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'd've\": 'it would have',\"it'll\": 'it will',\"it'll've\": 'it will have',\"it's\": 'it is','it’d': 'it would','it’d’ve': 'it would have','it’ll': 'it will','it’ll’ve': 'it will have','it’s': 'it is','jan.': 'january','jul.': 'july','jun.': 'june','kinda': 'kind of',\"let's\": 'let us','let’s': 'let us','luv': 'love',\"ma'am\": 'madam','mar.': 'march',\"may've\": 'may have',\"mayn't\": 'may not','mayn’t': 'may not','may’ve': 'may have','ma’am': 'madam',\"might've\": 'might have',\"mightn't\": 'might not',\"mightn't've\": 'might not have','mightn’t': 'might not','mightn’t’ve': 'might not have','might’ve': 'might have',\"must've\": 'must have',\"mustn't\": 'must not',\"mustn't've\": 'must not have','mustn’t': 'must not','mustn’t’ve': 'must not have','must’ve': 'must have',\"ne'er\": 'never',\"needn't\": 'need not',\"needn't've\": 'need not have','needn’t': 'need not','needn’t’ve': 'need not have','ne’er': 'never','nov.': 'november',\"o'\": 'of',\"o'clock\": 'of the clock',\"o'er\": 'over','oct.': 'october',\"ol'\": 'old','ol’': 'old',\"oughtn't\": 'ought not',\"oughtn't've\": 'ought not have','oughtn’t': 'ought not','oughtn’t’ve': 'ought not have','o’': 'of','o’clock': 'of the clock','o’er': 'over','sep.': 'september',\"sha'n't\": 'shall not',\"shalln't\": 'shall not','shalln’t': 'shall not',\"shan't\": 'shall not',\"shan't've\": 'shall not have','shan’t': 'shall not','shan’t’ve': 'shall not have','sha’n’t': 'shall not',\"she'd\": 'she would',\"she'd've\": 'she would have',\"she'll\": 'she will',\"she's\": 'she is','she’d': 'she would','she’d’ve': 'she would have','she’ll': 'she will','she’s': 'she is',\"should've\": 'should have',\"shouldn't\": 'should not',\"shouldn't've\": 'should not have','shouldn’t': 'should not','shouldn’t’ve': 'should not have','should’ve': 'should have',\"so's\": 'so is',\"so've\": 'so have',\"somebody's\": 'somebody is','somebody’s': 'somebody is',\"someone's\": 'someone is','someone’s': 'someone is',\"something's\": 'something is','something’s': 'something is','so’s': 'so is','so’ve': 'so have','sux': 'sucks',\"that'd\": 'that would',\"that'd've\": 'that would have',\"that'll\": 'that will',\"that're\": 'that are',\"that's\": 'that is','that’d': 'that would','that’d’ve': 'that would have','that’ll': 'that will','that’re': 'that are','that’s': 'that is',\"there'd\": 'there would',\"there'd've\": 'there would have',\"there'll\": 'there will',\"there're\": 'there are',\"there's\": 'there is','there’d': 'there would','there’d’ve': 'there would have','there’ll': 'there will','there’re': 'there are','there’s': 'there is',\"these're\": 'these are','these’re': 'these are',\"they'd\": 'they would',\"they'd've\": 'they would have',\"they'll\": 'they will',\"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', 'they’d': 'they would','they’d’ve': 'they would have', 'they’ll': 'they will', 'they’ll’ve': 'they will have', 'they’re': 'they are','they’ve': 'they have', \"this's\": 'this is', 'this’s': 'this is', \"those're\": 'those are','those’re': 'those are', \"to've\": 'to have', 'to’ve': 'to have', 'wanna': 'want to',\"wasn't\": 'was not', 'wasn’t': 'was not', \"we'd\": 'we would', \"we'd've\": 'we would have',\"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have',\"weren't\": 'were not', 'weren’t': 'were not', 'we’d': 'we would', 'we’d’ve': 'we would have','we’ll': 'we will', 'we’ll’ve': 'we will have', 'we’re': 'we are', 'we’ve': 'we have',\"what'd\": 'what did', \"what'll\": 'what will', \"what'll've\": 'what will have',\"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', 'what’d': 'what did','what’ll': 'what will', 'what’ll’ve': 'what will have', 'what’re': 'what are', 'what’s': 'what is','what’ve': 'what have', \"when's\": 'when is', \"when've\": 'when have', 'when’s': 'when is', 'when’ve': 'when have', \"where'd\": 'where did',\"where're\": 'where are', \"where's\": 'where is', \"where've\": 'where have', 'where’d': 'where did','where’re': 'where are', 'where’s': 'where is', 'where’ve': 'where have', \"which's\": 'which is','which’s': 'which is', \"who'd\": 'who would', \"who'd've\": 'who would have', \"who'll\": 'who will',\"who'll've\": 'who will have', \"who're\": 'who are', \"who's\": 'who is', \"who've\": 'who have','who’d': 'who would', 'who’d’ve': 'who would have', 'who’ll': 'who will', 'who’ll’ve': 'who will have','who’re': 'who are', 'who’s': 'who is', 'who’ve': 'who have', \"why'd\": 'why did',\"why're\": 'why are', \"why's\": 'why is', \"why've\": 'why have', 'why’d': 'why did','why’re': 'why are', 'why’s': 'why is', 'why’ve': 'why have', \"will've\": 'will have','will’ve': 'will have', \"won't\": 'will not', \"won't've\": 'will not have', 'won’t': 'will not','won’t’ve': 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have','wouldn’t': 'would not', 'wouldn’t’ve': 'would not have', 'would’ve': 'would have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have',\"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would', \"you'd've\": 'you would have', \"you'll\": 'you will', \"you'll've\": 'you shall have', \"you're\": 'you are', \"you've\": 'you have','you’d': 'you would', 'you’d’ve': 'you would have', 'you’ll': 'you will', 'you’ll’ve': 'you shall have', 'you’re': 'you are', 'you’ve': 'you have', 'y’all': 'you all', 'y’all’d': 'you all would', 'y’all’d’ve': 'you all would have','y’all’re': 'you all are', 'y’all’ve': 'you all have', '’cause': 'because', '’tis': 'it is', '’twas': 'it was'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contracted_word_expansion(token):\n",
    "    if token in contractions_dict.keys():\n",
    "        return contractions_dict[token]\n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You don't need a library\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contracted_word_expansion('You don\\'t need a library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractions_expansion(list_of_tokens):\n",
    "    return map(contracted_word_expansion,list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: contractions_expansion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now data came is a non contracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[enron, methanol, ;, meter, #, :, 988291, this...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[hpl, nom, for, january, 9, ,, 2001, (, see, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[neon, retreat, ho, ho, ho, ,, we, ', re, arou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[photoshop, ,, windows, ,, office, ., cheap, ....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[indian, springs, this, deal, is, to, book, th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>[put, the, 10, on, the, ft, the, transport, vo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>[3, /, 4, /, 2000, and, following, noms, hpl, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>[calpine, daily, gas, nomination, &gt;, &gt;, julie,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>[industrial, worksheets, for, august, 2000, ac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>[important, online, banking, alert, dear, valu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5171 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  spam\n",
       "0     [enron, methanol, ;, meter, #, :, 988291, this...     0\n",
       "1     [hpl, nom, for, january, 9, ,, 2001, (, see, a...     0\n",
       "2     [neon, retreat, ho, ho, ho, ,, we, ', re, arou...     0\n",
       "3     [photoshop, ,, windows, ,, office, ., cheap, ....     1\n",
       "4     [indian, springs, this, deal, is, to, book, th...     0\n",
       "...                                                 ...   ...\n",
       "5166  [put, the, 10, on, the, ft, the, transport, vo...     0\n",
       "5167  [3, /, 4, /, 2000, and, following, noms, hpl, ...     0\n",
       "5168  [calpine, daily, gas, nomination, >, >, julie,...     0\n",
       "5169  [industrial, worksheets, for, august, 2000, ac...     0\n",
       "5170  [important, online, banking, alert, dear, valu...     1\n",
       "\n",
       "[5171 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we remove unnecessary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'^@[a-zA-z0-9]|^#[a-zA-Z0-9]|\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*|\\W+|\\d+|<(\"[^\"]*\"|\\'[^\\']*\\'|[^\\'\">])*>|_+|[^\\u0000-\\u007f]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waste_word_or_not(token):\n",
    "    return re.search(regex,token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_waste_words(list_of_tokens):\n",
    "    return filterfalse(waste_word_or_not,list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: filter_waste_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [enron, methanol, meter, this, is, a, follow, ...\n",
       "1       [hpl, nom, for, january, see, attached, file, ...\n",
       "2       [neon, retreat, ho, ho, ho, we, re, around, to...\n",
       "3       [photoshop, windows, office, cheap, main, tren...\n",
       "4       [indian, springs, this, deal, is, to, book, th...\n",
       "                              ...                        \n",
       "5166    [put, the, on, the, ft, the, transport, volume...\n",
       "5167    [and, following, noms, hpl, can, t, take, the,...\n",
       "5168    [calpine, daily, gas, nomination, julie, as, i...\n",
       "5169    [industrial, worksheets, for, august, activity...\n",
       "5170    [important, online, banking, alert, dear, valu...\n",
       "Name: text, Length: 5171, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(list_of_tokens):\n",
    "    return map(lambda x: re.split(regex,x)[0],list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: split(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [enron, methanol, meter, this, is, a, follow, ...\n",
       "1       [hpl, nom, for, january, see, attached, file, ...\n",
       "2       [neon, retreat, ho, ho, ho, we, re, around, to...\n",
       "3       [photoshop, windows, office, cheap, main, tren...\n",
       "4       [indian, springs, this, deal, is, to, book, th...\n",
       "                              ...                        \n",
       "5166    [put, the, on, the, ft, the, transport, volume...\n",
       "5167    [and, following, noms, hpl, can, t, take, the,...\n",
       "5168    [calpine, daily, gas, nomination, julie, as, i...\n",
       "5169    [industrial, worksheets, for, august, activity...\n",
       "5170    [important, online, banking, alert, dear, valu...\n",
       "Name: text, Length: 5171, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We r going to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop_words = list(set(stopwords.words('english')).union(set(STOP_WORDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stopword(token):\n",
    "    return not(token in en_stop_words or re.search(r'\\b\\w\\b|[^\\u0000-\\u007f]+|_+|\\W+',token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_removals(list_of_tokens):\n",
    "    return filter(is_stopword,list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: stopwords_removals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [enron, methanol, meter, follow, note, gave, m...\n",
       "1       [hpl, nom, january, attached, file, hplnol, xl...\n",
       "2       [neon, retreat, ho, ho, ho, wonderful, time, y...\n",
       "3       [photoshop, windows, office, cheap, main, tren...\n",
       "4       [indian, springs, deal, book, teco, pvr, reven...\n",
       "                              ...                        \n",
       "5166    [ft, transport, volumes, decreased, contract, ...\n",
       "5167    [following, noms, hpl, extra, mmcf, weekend, t...\n",
       "5168    [calpine, daily, gas, nomination, julie, menti...\n",
       "5169    [industrial, worksheets, august, activity, att...\n",
       "5170    [important, online, banking, alert, dear, valu...\n",
       "Name: text, Length: 5171, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we r going to perform limitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wnet_pos_tag(treebank_tag):\n",
    "    if treebank_tag[1].startswith('J'):\n",
    "        return (treebank_tag[0],wordnet.ADJ)\n",
    "    elif treebank_tag[1].startswith('V'):\n",
    "        return (treebank_tag[0],wordnet.VERB)\n",
    "    elif treebank_tag[1].startswith('N'):\n",
    "        return (treebank_tag[0],wordnet.NOUN)\n",
    "    elif treebank_tag[1].startswith('R'):\n",
    "        return (treebank_tag[0],wordnet.ADV)\n",
    "    else:\n",
    "        (treebank_tag[0],wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(list_of_tokens):\n",
    "    if len(list_of_tokens)> 0:\n",
    "        return map(get_wnet_pos_tag,pos_tag(list_of_tokens))\n",
    "    else:\n",
    "        return[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: get_pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\uic19\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\uic19\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(enron, n), (methanol, n), (meter, n), (follo...\n",
       "1       [(hpl, n), None, (january, a), (attached, v), ...\n",
       "2       [(neon, r), (retreat, n), (ho, a), (ho, n), (h...\n",
       "3       [(photoshop, n), (windows, v), (office, n), (c...\n",
       "4       [(indian, a), (springs, n), (deal, n), (book, ...\n",
       "                              ...                        \n",
       "5166    [(ft, a), (transport, n), (volumes, n), (decre...\n",
       "5167    [(following, v), (noms, n), (hpl, a), (extra, ...\n",
       "5168    [(calpine, a), (daily, a), (gas, n), (nominati...\n",
       "5169    [(industrial, a), (worksheets, n), (august, v)...\n",
       "5170    [(important, a), (online, n), (banking, n), (a...\n",
       "Name: text, Length: 5171, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we r going to apply Limitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lemmatization(token):\n",
    "    if token == None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word=token[0],pos=token[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(list_of_tokens):\n",
    "    if len(list_of_tokens) > 0:\n",
    "        return map(lambda x: token_lemmatization(x),list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: lemmatization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       <map object at 0x00000241688C3A00>\n",
       "1       <map object at 0x00000241688C3BE0>\n",
       "2       <map object at 0x00000241688C3A30>\n",
       "3       <map object at 0x00000241688C39A0>\n",
       "4       <map object at 0x00000241688C37C0>\n",
       "                       ...                \n",
       "5166    <map object at 0x000002416D2D9CD0>\n",
       "5167    <map object at 0x000002416D2D9D60>\n",
       "5168    <map object at 0x000002416D2D9DF0>\n",
       "5169    <map object at 0x000002416D2D9E80>\n",
       "5170    <map object at 0x000002416D2D9F10>\n",
       "Name: text, Length: 5171, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-fdff4e7b74c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4198\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4200\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-fdff4e7b74c3>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for list_of_tokens in data['text']:\n",
    "    vocab = vocab.union(set(list_of_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = dict(zip(vocab,list(range(0,len(vocab)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we r joinng tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(list_of_tokens):\n",
    "    return \" \".join(list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: join_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list()\n",
    "for email_text in data['text']:\n",
    "    corpus.append(email_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(vocabulary=vocab_dict)\n",
    "tf_idf_matrix = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix = tf_idf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spam'] = data['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix_reduced = pca.fit_transform(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=tf_idf_matrix_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spam'] = data['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.iloc[:,0:5000]\n",
    "y_train = df['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_categories = gnb.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_train,y_pred=predicted_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
